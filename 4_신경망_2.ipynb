{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_신경망_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "H-uPBCIa-ROH"
      ],
      "authorship_tag": "ABX9TyO8KHVXiLRu5gyT9ApD1G8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ixxxxu/deep-learning-from-scratch/blob/master/4_%EC%8B%A0%EA%B2%BD%EB%A7%9D_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-uPBCIa-ROH"
      },
      "source": [
        "#### 한글폰트 설치방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOuj_8j5j2T"
      },
      "source": [
        "# #한글폰트설치\r\n",
        "# !apt-get update -qq\r\n",
        "# !apt-get install fonts-nanum* -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41s_aqyN292P"
      },
      "source": [
        "# # 폰트매니저 리빌드\r\n",
        "# import matplotlib.font_manager as fm\r\n",
        "# fm._rebuild()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHLCXjcH5eTV"
      },
      "source": [
        "# # 한글폰트 설치 확인\r\n",
        "# for fontInfo in fm.fontManager.ttflist:\r\n",
        "#   if 'Nanum' in fontInfo.name:\r\n",
        "#     print(fontInfo.name + \" = \" + fontInfo.fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoJWppJs6WVA"
      },
      "source": [
        "# # 한글폰트 설정\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "# plt.rc('font', family='NanumGothic') # 나눔고딕으로 한글 폰트 설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bvIf2dEvNHd"
      },
      "source": [
        "#### MNIST 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8KIlyYirwzn"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "mnist = tf.keras.datasets.mnist\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
        "\r\n",
        "print(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOy8CuxC8yZl"
      },
      "source": [
        "#### 예제 실습용 메소드\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWxrMPqEr6QQ"
      },
      "source": [
        "# common/functions.py \r\n",
        "  # softmax()\r\n",
        "  # cross_entropy_error()\r\n",
        "\r\n",
        "# coding: utf-8\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def identity_function(x):\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def step_function(x):\r\n",
        "    return np.array(x > 0, dtype=np.int)\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + np.exp(-x))    \r\n",
        "\r\n",
        "\r\n",
        "def sigmoid_grad(x):\r\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\r\n",
        "    \r\n",
        "\r\n",
        "def relu(x):\r\n",
        "    return np.maximum(0, x)\r\n",
        "\r\n",
        "\r\n",
        "def relu_grad(x):\r\n",
        "    grad = np.zeros(x)\r\n",
        "    grad[x>=0] = 1\r\n",
        "    return grad\r\n",
        "    \r\n",
        "\r\n",
        "def softmax(x):\r\n",
        "    if x.ndim == 2:\r\n",
        "        x = x.T\r\n",
        "        x = x - np.max(x, axis=0)\r\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
        "        return y.T \r\n",
        "\r\n",
        "    x = x - np.max(x) # 오버플로 대책\r\n",
        "    return np.exp(x) / np.sum(np.exp(x))\r\n",
        "\r\n",
        "\r\n",
        "def mean_squared_error(y, t):\r\n",
        "    return 0.5 * np.sum((y-t)**2)\r\n",
        "\r\n",
        "\r\n",
        "def cross_entropy_error(y, t):\r\n",
        "    if y.ndim == 1:\r\n",
        "        t = t.reshape(1, t.size)\r\n",
        "        y = y.reshape(1, y.size)\r\n",
        "        \r\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\r\n",
        "    if t.size == y.size:\r\n",
        "        t = t.argmax(axis=1)\r\n",
        "             \r\n",
        "    batch_size = y.shape[0]\r\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\r\n",
        "\r\n",
        "\r\n",
        "def softmax_loss(X, t):\r\n",
        "    y = softmax(X)\r\n",
        "    return cross_entropy_error(y, t)\r\n",
        "# common/gradient.py\r\n",
        "  # numerical_gradient()\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def _numerical_gradient_1d(f, x):\r\n",
        "    h = 1e-4 # 0.0001\r\n",
        "    grad = np.zeros_like(x)\r\n",
        "    \r\n",
        "    for idx in range(x.size):\r\n",
        "        tmp_val = x[idx]\r\n",
        "        x[idx] = float(tmp_val) + h\r\n",
        "        fxh1 = f(x) # f(x+h)\r\n",
        "        \r\n",
        "        x[idx] = tmp_val - h \r\n",
        "        fxh2 = f(x) # f(x-h)\r\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n",
        "        \r\n",
        "        x[idx] = tmp_val # 값 복원\r\n",
        "        \r\n",
        "    return grad\r\n",
        "\r\n",
        "\r\n",
        "def numerical_gradient_2d(f, X):\r\n",
        "    if X.ndim == 1:\r\n",
        "        return _numerical_gradient_1d(f, X)\r\n",
        "    else:\r\n",
        "        grad = np.zeros_like(X)\r\n",
        "        \r\n",
        "        for idx, x in enumerate(X):\r\n",
        "            grad[idx] = _numerical_gradient_1d(f, x)\r\n",
        "        \r\n",
        "        return grad\r\n",
        "\r\n",
        "\r\n",
        "def numerical_gradient(f, x):\r\n",
        "    h = 1e-4 # 0.0001\r\n",
        "    grad = np.zeros_like(x)\r\n",
        "    \r\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\r\n",
        "    while not it.finished:\r\n",
        "        idx = it.multi_index\r\n",
        "        tmp_val = x[idx]\r\n",
        "        x[idx] = float(tmp_val) + h\r\n",
        "        fxh1 = f(x) # f(x+h)\r\n",
        "        \r\n",
        "        x[idx] = tmp_val - h \r\n",
        "        fxh2 = f(x) # f(x-h)\r\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n",
        "        \r\n",
        "        x[idx] = tmp_val # 값 복원\r\n",
        "        it.iternext()   \r\n",
        "        \r\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcUlFivyFev5"
      },
      "source": [
        "#### 지난시간에\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj0VvjXSjmDf"
      },
      "source": [
        " **학습이란** \r\n",
        "- 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\r\n",
        "\r\n",
        "**손실함수** 란\r\n",
        "- 신경망이 학습할 수 있도록 해 주는 **지표**\r\n",
        "\r\n",
        "\r\n",
        "**4.1.1 데이터 **주도** 학습**\r\n",
        "- 데이터에서 **특징(Feature)**를 추출하고, 그 특징의 패턴을 기계가 학습하는 것 (지도/비지도/강화학습 등)\r\n",
        "\r\n",
        "**4.1.2 훈련데이터와 시험데이터**\r\n",
        "- 학습모델의 평가를 위해 훈련데이터와 시험데이터를 분리\r\n",
        "  - 오버피팅(Over Fitting 과대적합) : 한 데이터셋에만 지나치게 최적화된 상태  \r\n",
        "\r\n",
        "**4.2 손실함수(Loss Function)**\r\n",
        "- 학습모델성능의 '나쁨'을 나타내는 지표 \r\n",
        "    - **평균 제곱 오차 MSE**\r\n",
        "      - 소프트맥스 함수  \r\n",
        "\r\n",
        "    - **교차 엔트로피 오차 CEE**\r\n",
        "      - 로그\r\n",
        "\r\n",
        "**4.2.3 미니배치 학습**\r\n",
        "- e.g. 훈련데이터가 100개 == 손실 함수의 값 100개\r\n",
        "  - 모든 데이터의 손실함수를 모두 구하는 것은 비효율적\r\n",
        "  - 평균손실 함수를 구하여 사용\r\n",
        "  - 미니배치 학습\r\n",
        "    - 훈련 데이터로부터 일부만 골라 학습을 수행\r\n",
        "\r\n",
        "**4.2.4 교차 엔트로피 오차 구현**\r\n",
        "\r\n",
        "**4.2.5 왜 손실 함수를 설정하는가 ?**\r\n",
        "- 높은 '정확도'를 끌어내는 매개변수(가중치와 편향) 값을 찾기 위함\r\n",
        "  - 미분을 통하여 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾음\r\n",
        "  - 반대로 정확도를 지표삼아 미분을 하면 대부분의 장소에서 0이 됨 (신경망 학습에서 계단함수를 쓰지 않는 이유)\r\n",
        "  - 신경망학습에서 중요한 성질은 , 기울기가 0이 되지 않게 하는것 (신경망 학습에서 시그모이드 함수를 쓰는 이유)\r\n",
        "\r\n",
        "**4.3 수치 미분**\r\n",
        "- 경사법 = 기울기(경사) 값을 기준으로 나아갈 방향을 정함\r\n",
        "\r\n",
        "**4.3.1 미분**\r\n",
        "- 한순간의 변화량\r\n",
        "- 수치 미분\r\n",
        "  - 아주 작은 차분으로 미분 하는 것\r\n",
        "  - 반올림 오차 rounding error : 너무 작은 값을 컴퓨터로 계산하는 데 문제가 됨\r\n",
        "    - '근사치'로 계산하는 방법\r\n",
        "      - 중심차분 / 중앙 차분 : x를 중심으로 그 전후의 차분을 계산\r\n",
        "      - 수전방 차분 : (x+h)와 x의 차분\r\n",
        "        - 차분 differencing : 연이은 관측값들의 차이를 계산하는 것\r\n",
        "\r\n",
        "- 해석적 미분 : 수식을 전개하여 미분하는 것\r\n",
        "\r\n",
        "**4.3.2 수치 미분의 예**\r\n",
        "\r\n",
        "**4.3.3 편미분**\r\n",
        "- 변수가 여럿인 함수에 대한 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZarFsIb1IB0"
      },
      "source": [
        "#### 4.4기울기 \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zL3vRt90_-v"
      },
      "source": [
        "# coding: utf-8\r\n",
        "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pylab as plt\r\n",
        "from mpl_toolkits.mplot3d import Axes3D\r\n",
        "\r\n",
        "\r\n",
        "def _numerical_gradient_no_batch(f, x):\r\n",
        "    h = 1e-4 # 0.0001\r\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\r\n",
        "    \r\n",
        "    for idx in range(x.size):\r\n",
        "        tmp_val = x[idx]\r\n",
        "        \r\n",
        "        # f(x+h) 계산\r\n",
        "        x[idx] = float(tmp_val) + h\r\n",
        "        fxh1 = f(x)\r\n",
        "        \r\n",
        "        # f(x-h) 계산\r\n",
        "        x[idx] = tmp_val - h \r\n",
        "        fxh2 = f(x) \r\n",
        "        \r\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n",
        "        x[idx] = tmp_val # 값 복원\r\n",
        "        \r\n",
        "    return grad\r\n",
        "\r\n",
        "\r\n",
        "def numerical_gradient(f, X):\r\n",
        "    if X.ndim == 1:\r\n",
        "        return _numerical_gradient_no_batch(f, X)\r\n",
        "    else:\r\n",
        "        grad = np.zeros_like(X)\r\n",
        "        \r\n",
        "        for idx, x in enumerate(X):\r\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\r\n",
        "        \r\n",
        "        return grad\r\n",
        "\r\n",
        "def function_2(x):\r\n",
        "    if x.ndim == 1:\r\n",
        "        return np.sum(x**2)\r\n",
        "    else:\r\n",
        "        return np.sum(x**2, axis=1)\r\n",
        "\r\n",
        "\r\n",
        "def tangent_line(f, x):\r\n",
        "    d = numerical_gradient(f, x)\r\n",
        "    print(d)\r\n",
        "    y = f(x) - d*x\r\n",
        "    return lambda t: d*t + y\r\n",
        "     \r\n",
        "\r\n",
        "display(numerical_gradient(function_2, np.array([3.0, 4.0])))\r\n",
        "display(numerical_gradient(function_2, np.array([0.0, 2.0])))\r\n",
        "display(numerical_gradient(function_2, np.array([3.0, 0.0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzri4gJD7L71"
      },
      "source": [
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    x0 = np.arange(-2, 2.5, 0.25)\r\n",
        "    x1 = np.arange(-2, 2.5, 0.25)\r\n",
        "    X, Y = np.meshgrid(x0, x1)\r\n",
        "    \r\n",
        "    X = X.flatten()\r\n",
        "    Y = Y.flatten()\r\n",
        "    \r\n",
        "    grad = numerical_gradient(function_2, np.array([X, Y]) )\r\n",
        "    \r\n",
        "    plt.figure(figsize=(10,10))\r\n",
        "    plt.subplot(2,2,1)\r\n",
        "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\r\n",
        "    plt.xlim([-2, 2])\r\n",
        "    plt.ylim([-2, 2])\r\n",
        "    plt.xlabel('x0')\r\n",
        "    plt.ylabel('x1')\r\n",
        "    plt.title(\"<f(x0,x1 = x0 **2 + x1**2>\")\r\n",
        "\r\n",
        "    plt.subplot(2,2,2)\r\n",
        "    plt.xlim([-2, 2])\r\n",
        "    plt.ylim([-2, 2])\r\n",
        "    plt.xlabel('x0')\r\n",
        "    plt.ylabel('x1')\r\n",
        "    plt.title(\"<f(x0,x1 = x0 **2 + x1**2>\")\r\n",
        "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",headwidth=10,scale=40,color = 'r')\r\n",
        "    plt.grid()\r\n",
        "    # plt.legend()\r\n",
        "    plt.draw()\r\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZob0wui9yo-"
      },
      "source": [
        "  - #### 기울기가 가르키는 쪽은 각 장소에서 **함수의 출력값**을 **가장 크게 줄이는 방향**을 타나냄\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrp3gDbu2Sf9"
      },
      "source": [
        "#### 4.4.1 경사하강법\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzGoeBUq-zFf"
      },
      "source": [
        "- 안장점\r\n",
        "- 고원\r\n",
        "- 경사법\r\n",
        "- 경사 하강법 \r\n",
        "- 경사 상승법\r\n",
        "- **학습률 Learning rate** : 한번의 학습으로 얼마만큼 학습해야 할지(매개변수 값을 얼마나 갱신할지)를 정하는 것\r\n",
        "  - 0.01 - 0.001 등 특정 값을 선택\r\n",
        "  - 너무 크거나 작으면 '**좋은 장소**'를 찾아 갈 수 없음 \r\n",
        "    - **좋은 장소**란 무엇일까요 ?\r\n",
        "\r\n",
        "- **하이퍼파라미터 Hyper Parameter** (초매개변수)\r\n",
        "  - 가중치와 편향 같은 신경망의 매개변수와는 성질 이 다른 매개변수\r\n",
        "  - 신경망의 가중치 매개변수는 훈련데이터와 학습 알고리즘 에 의해서 '자동'으로 획득됨\r\n",
        "  - 학습률 같은 하이퍼파라미터는 **사람이 직접 설정해야 하는 매개변수** \r\n",
        "  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGTFNnYT_W7t"
      },
      "source": [
        "# coding: utf-8\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pylab as plt\r\n",
        "\r\n",
        "\r\n",
        "# 경사하강법 함수 구현\r\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\r\n",
        "    x = init_x\r\n",
        "    x_history = []\r\n",
        "\r\n",
        "    for i in range(step_num):\r\n",
        "        x_history.append( x.copy() )\r\n",
        "\r\n",
        "        grad = numerical_gradient(f, x)\r\n",
        "        x -= lr * grad\r\n",
        "\r\n",
        "    return x, np.array(x_history)\r\n",
        "\r\n",
        "\r\n",
        "def function_2(x):\r\n",
        "    return x[0]**2 + x[1]**2\r\n",
        "\r\n",
        "# 경사하강법 함수 시각화\r\n",
        "init_x = np.array([-3.0, 4.0]) # 초기값 (-3.0 , 4.0) \r\n",
        "\r\n",
        "lr = 0.1 # Learning rate 학습률\r\n",
        "step_num = 20\r\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\r\n",
        "\r\n",
        "plt.plot( [-5, 5], [0,0], '--b')\r\n",
        "plt.plot( [0,0], [-5, 5], '--b')\r\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\r\n",
        "\r\n",
        "plt.xlim(-3.5, 3.5)\r\n",
        "plt.ylim(-4.5, 4.5)\r\n",
        "plt.xlabel(\"X0\")\r\n",
        "plt.ylabel(\"X1\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMVGhisEMR7"
      },
      "source": [
        "init_x = np.array([-3.0, 4.0])    \r\n",
        "\r\n",
        "lr = 0.1\r\n",
        "step_num = 20\r\n",
        "gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\r\n",
        "\r\n",
        "#최소값 (0,0)에 가까운 값을 경사법으로 얻은것을 확인 할 수 있다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E-dCkekEPQ1"
      },
      "source": [
        "# 학습률이 너무 클때\r\n",
        "init_x = np.array([-3.0, 4.0])    \r\n",
        "\r\n",
        "lr = 10.0\r\n",
        "step_num = 20\r\n",
        "gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\r\n",
        "\r\n",
        "# >>> 너무 큰값을 발산"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsQeXhD6Elp0"
      },
      "source": [
        "# 학습률이 너무 작을때\r\n",
        "init_x = np.array([-3.0, 4.0])    \r\n",
        "\r\n",
        "lr = 1e-10\r\n",
        "step_num = 20\r\n",
        "gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\r\n",
        "\r\n",
        "# >>> 거의 갱신되지 않은채 끝남"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEqlH6qqFAX-"
      },
      "source": [
        "# 경사하강법 구현 함수\r\n",
        "\r\n",
        "# coding: utf-8\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pylab as plt\r\n",
        "# from gradient_2d import numerical_gradient\r\n",
        "\r\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\r\n",
        "    x = init_x\r\n",
        "    x_history = []\r\n",
        "\r\n",
        "    for i in range(step_num):\r\n",
        "        x_history.append( x.copy() )\r\n",
        "\r\n",
        "        grad = numerical_gradient(f, x)\r\n",
        "        x -= lr * grad\r\n",
        "\r\n",
        "    return x, np.array(x_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhOi9Ec6DxZA"
      },
      "source": [
        "# f(x0,x1) = x0 **2 + x1 **2 구하기\r\n",
        "\r\n",
        "def function_2(x):\r\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGldZacmFNv8"
      },
      "source": [
        "#### 4.4.2 신경망에서의 기울기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bhaM8x5FS_R"
      },
      "source": [
        "# coding: utf-8\r\n",
        "# import sys, os\r\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n",
        "import numpy as np\r\n",
        "# from common.functions import softmax, cross_entropy_error\r\n",
        "# from common.gradient import numerical_gradient\r\n",
        "\r\n",
        "\r\n",
        "class simpleNet:\r\n",
        "    def __init__(self):\r\n",
        "        self.W = np.random.randn(2,3) # 정규분포로 초기화\r\n",
        "\r\n",
        "    def predict(self, x):\r\n",
        "        return np.dot(x, self.W)\r\n",
        "\r\n",
        "    def loss(self, x, t):\r\n",
        "        z = self.predict(x)\r\n",
        "        y = softmax(z)\r\n",
        "        loss = cross_entropy_error(y, t)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "x = np.array([0.6, 0.9])\r\n",
        "t = np.array([0, 0, 1])\r\n",
        "\r\n",
        "net = simpleNet()\r\n",
        "\r\n",
        "f = lambda w: net.loss(x, t)\r\n",
        "dW = numerical_gradient(f, net.W)\r\n",
        "\r\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQyJPCB9FUJ4"
      },
      "source": [
        "- 신경망 학습에서도 기울기를 구해야함\r\n",
        "- 가중치 매개변수에 대한 손실 함수의 기울기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNM7GwuVB0Lm"
      },
      "source": [
        "net = simpleNet()\r\n",
        "print(net.W) # 가중치 매개변수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnY5crbYB-mk"
      },
      "source": [
        "x = np.array([0.6,.09])\r\n",
        "p = net.predict(x)\r\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCn0MSmpCIsf"
      },
      "source": [
        "np.argmax(p) # 최댓값의 인덱스"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqQBHnb9CPYU"
      },
      "source": [
        "t = np.array([0,0,1])\r\n",
        "net.loss(x,t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4MwMAKKCa18"
      },
      "source": [
        "# 기울기 값 구하기\r\n",
        "def f(W):\r\n",
        "  return net.loss(x,t)\r\n",
        "dW = numerical_gradient(f,net.W)\r\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtdGZninL91"
      },
      "source": [
        "#### 4.5 학습알고리즘 구현하기\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtLPKeAhnUwH"
      },
      "source": [
        "전제\r\n",
        "- 신경망에 적응 가능한 가중치와 편향이 있다\r\n",
        "- 이 가중치와 편향ㅇ르 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다\r\n",
        "\r\n",
        "신경망 학습 단계 (확률적 경사 하강법 stochastic gradient descent의 예시)\r\n",
        "1.   미니 배치\r\n",
        "  - 훈련 데이터중 일부를 무작위 선별한 데이터\r\n",
        "  - 미니배치의 손실함수 값을 줄이는것을 목표로 함\r\n",
        "2.   기울기 산출\r\n",
        "  - 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 산출\r\n",
        "  - 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시\r\n",
        "3.   매개변수 갱신\r\n",
        "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\r\n",
        "4.   1~3단계 반복\r\n",
        "\r\n",
        "확률적 경사 하강법\r\n",
        "- 미니배치를 통한 무작위 확률적 데이터에 대한 수행 방법이라 하여 SGD라고도 함\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXGoW4R9pNBe"
      },
      "source": [
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opDxjtpfpN1n"
      },
      "source": [
        "#### 4.5.1 2층 신경망 클래스 구현하기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQI4kJBx9O4S"
      },
      "source": [
        "#  TwoLayerNet 클래스 \r\n",
        "\r\n",
        "# coding: utf-8\r\n",
        "# import sys, os\r\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n",
        "# from common.functions import *\r\n",
        "# from common.gradient import numerical_gradient\r\n",
        "\r\n",
        "\r\n",
        "class TwoLayerNet:\r\n",
        "\r\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\r\n",
        "        # 가중치 초기화\r\n",
        "        self.params = {}\r\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "        self.params['b1'] = np.zeros(hidden_size)\r\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\r\n",
        "        self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "    def predict(self, x):\r\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\r\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "    \r\n",
        "        a1 = np.dot(x, W1) + b1\r\n",
        "        z1 = sigmoid(a1)\r\n",
        "        a2 = np.dot(z1, W2) + b2\r\n",
        "        y = softmax(a2)\r\n",
        "        \r\n",
        "        return y\r\n",
        "        \r\n",
        "    # x : 입력 데이터, t : 정답 레이블\r\n",
        "    def loss(self, x, t):\r\n",
        "        y = self.predict(x)\r\n",
        "        \r\n",
        "        return cross_entropy_error(y, t)\r\n",
        "    \r\n",
        "    def accuracy(self, x, t):\r\n",
        "        y = self.predict(x)\r\n",
        "        y = np.argmax(y, axis=1)\r\n",
        "        t = np.argmax(t, axis=1)\r\n",
        "        \r\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "        return accuracy\r\n",
        "        \r\n",
        "    # x : 입력 데이터, t : 정답 레이블\r\n",
        "    def numerical_gradient(self, x, t):\r\n",
        "        loss_W = lambda W: self.loss(x, t)\r\n",
        "        \r\n",
        "        grads = {}\r\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\r\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\r\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\r\n",
        "        \r\n",
        "        return grads\r\n",
        "        \r\n",
        "    def gradient(self, x, t):\r\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\r\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "        grads = {}\r\n",
        "        \r\n",
        "        batch_num = x.shape[0]\r\n",
        "        \r\n",
        "        # forward\r\n",
        "        a1 = np.dot(x, W1) + b1\r\n",
        "        z1 = sigmoid(a1)\r\n",
        "        a2 = np.dot(z1, W2) + b2\r\n",
        "        y = softmax(a2)\r\n",
        "        \r\n",
        "        # backward\r\n",
        "        dy = (y - t) / batch_num\r\n",
        "        grads['W2'] = np.dot(z1.T, dy)\r\n",
        "        grads['b2'] = np.sum(dy, axis=0)\r\n",
        "        \r\n",
        "        da1 = np.dot(dy, W2.T)\r\n",
        "        dz1 = sigmoid_grad(a1) * da1\r\n",
        "        grads['W1'] = np.dot(x.T, dz1)\r\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\r\n",
        "\r\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWtdGg-kpTjy"
      },
      "source": [
        "# coding: utf-8\r\n",
        "# import sys, os\r\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# from two_layer_net import TwoLayerNet\r\n",
        "\r\n",
        "# 데이터 읽기\r\n",
        "\r\n",
        "\r\n",
        "(x_train, t_train), (x_test, t_test) = mnist(normalize=True, one_hot_label=True)\r\n",
        "\r\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\r\n",
        "\r\n",
        "# 하이퍼파라미터\r\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\r\n",
        "train_size = x_train.shape[0]\r\n",
        "batch_size = 100   # 미니배치 크기\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "train_loss_list = []\r\n",
        "train_acc_list = []\r\n",
        "test_acc_list = []\r\n",
        "\r\n",
        "# 1에폭당 반복 수\r\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\r\n",
        "\r\n",
        "for i in range(iters_num):\r\n",
        "    # 미니배치 획득\r\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\r\n",
        "    x_batch = x_train[batch_mask]\r\n",
        "    t_batch = t_train[batch_mask]\r\n",
        "    \r\n",
        "    # 기울기 계산\r\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\r\n",
        "    grad = network.gradient(x_batch, t_batch)\r\n",
        "    \r\n",
        "    # 매개변수 갱신\r\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\r\n",
        "        network.params[key] -= learning_rate * grad[key]\r\n",
        "    \r\n",
        "    # 학습 경과 기록\r\n",
        "    loss = network.loss(x_batch, t_batch)\r\n",
        "    train_loss_list.append(loss)\r\n",
        "    \r\n",
        "    # 1에폭당 정확도 계산\r\n",
        "    if i % iter_per_epoch == 0:\r\n",
        "        train_acc = network.accuracy(x_train, t_train)\r\n",
        "        test_acc = network.accuracy(x_test, t_test)\r\n",
        "        train_acc_list.append(train_acc)\r\n",
        "        test_acc_list.append(test_acc)\r\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\r\n",
        "\r\n",
        "# 그래프 그리기\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, label='train acc')\r\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIBp-RSw91Sq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aixJO1fpAe5d"
      },
      "source": [
        "#### 4.5.3 시험데이터로 평가하기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DtigenjAiII"
      },
      "source": [
        "# coding: utf-8\r\n",
        "import sys, os\r\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from dataset.mnist import load_mnist\r\n",
        "from two_layer_net import TwoLayerNet\r\n",
        "\r\n",
        "# 데이터 읽기\r\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\r\n",
        "\r\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\r\n",
        "\r\n",
        "# 하이퍼파라미터\r\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\r\n",
        "train_size = x_train.shape[0]\r\n",
        "batch_size = 100   # 미니배치 크기\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "train_loss_list = []\r\n",
        "train_acc_list = []\r\n",
        "test_acc_list = []\r\n",
        "\r\n",
        "# 1에폭당 반복 수\r\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\r\n",
        "\r\n",
        "for i in range(iters_num):\r\n",
        "    # 미니배치 획득\r\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\r\n",
        "    x_batch = x_train[batch_mask]\r\n",
        "    t_batch = t_train[batch_mask]\r\n",
        "    \r\n",
        "    # 기울기 계산\r\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\r\n",
        "    grad = network.gradient(x_batch, t_batch)\r\n",
        "    \r\n",
        "    # 매개변수 갱신\r\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\r\n",
        "        network.params[key] -= learning_rate * grad[key]\r\n",
        "    \r\n",
        "    # 학습 경과 기록\r\n",
        "    loss = network.loss(x_batch, t_batch)\r\n",
        "    train_loss_list.append(loss)\r\n",
        "    \r\n",
        "    # 1에폭당 정확도 계산\r\n",
        "    if i % iter_per_epoch == 0:\r\n",
        "        train_acc = network.accuracy(x_train, t_train)\r\n",
        "        test_acc = network.accuracy(x_test, t_test)\r\n",
        "        train_acc_list.append(train_acc)\r\n",
        "        test_acc_list.append(test_acc)\r\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\r\n",
        "\r\n",
        "# 그래프 그리기\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, label='train acc')\r\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}